{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функциональные требования\n",
    "\n",
    "Входными данными для системы является текстовый файл, содержащий текст на русском языке. Входной текст не имеет строго\n",
    "заданного формата.\n",
    "\n",
    "Система должна соответствовать следующим требованиям:\n",
    " 1. выделение в исходном тексте предложений;\n",
    " 2. токенизация исходного текста (выделение в тексте слов, чисел, и иных токенов, в том числе, например, нахождение границ предложений);\n",
    " 3. поиск в тексте сокращений, аббревиатур;\n",
    " 4. определение расшифровок для найденных сокращений и аббревиатур;\n",
    " 5. возможность поиска текстовых конструкций с использованием шаблонов;\n",
    " 6. возможность добавления новых шаблонов для поиска;\n",
    " 7. простейшие средства визуализации;\n",
    " 8. система должна функционировать без предварительного машинного обучения на корректно обработанной экспертами коллекции текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходные данные должны быть представлены в формате, пригодном для передачи на следующий этап – морфологический анализ, и содержать следующую информацию:\n",
    "* список выделенных в тексте предложений;\n",
    "* список найденных графематических дескрипторов.\n",
    "\n",
    "В списке предложений должны содержаться:\n",
    "* позиция начала предложения в тексте;\n",
    "* его длина (количество символов);\n",
    "* текст предложения.\n",
    "\n",
    "Список графематических дескрипторов должен содержать в себе\n",
    "следующие данные:\n",
    "* тип дескриптора;\n",
    "* позиция начала дескриптора в тексте;\n",
    "* его длина (количество символов);\n",
    "* текст дескриптора;\n",
    "* информацию о том, может ли данный дескриптор стоять в конце предложения;\n",
    "* семантическую информацию о дескрипторе (для сокращений и аббревиатур)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "ListWithDescriptors = []\n",
    "text = 'Постсинтаксический анализ решает две следующие задачи. Первая заключается в необходимости уточнить смысл слов, который выражается с помощью различных средств языка: предлогов, префиксов и т.д. Проблематика второй задачи заключается в том, что одну и ту же мысль можно выразить разными конструкциями языка. Например, в многоязычно диалоговой системе одна и та же мысль может быть выражена различными синтаксическими конструкциями. В связи с этим появляется необходимость в нормализации дерева, чтобы свести конструкции, которые выражают одно действие различным образом для разных ситуаций, к одному нормализованному дереву. Так же на этапе постсинтаксического анализа может проводиться поиск изменяемых словосочетаний, составные части (слова) которых могут быть разделены другими словами [2].'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск ФИО, когда ФИО в сокращении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Постсинтаксический анализ решает две следующие задачи. Первая заключается в необходимости уточнить смысл слов, который выражается с помощью различных средств языка: предлогов, префиксов и т.д. Проблематика второй задачи заключается в том, что одну и ту же мысль можно выразить разными конструкциями языка. Например, в многоязычно диалоговой системе одна и та же мысль может быть выражена различными синтаксическими конструкциями. В связи с этим появляется необходимость в нормализации дерева, чтобы свести конструкции, которые выражают одно действие различным образом для разных ситуаций, к одному нормализованному дереву. Так же на этапе постсинтаксического анализа может проводиться поиск изменяемых словосочетаний, составные части (слова) которых могут быть разделены другими словами [2].\n"
     ]
    }
   ],
   "source": [
    "FIO = {'DescriptorName': 'FIO', 'TextDescriptor': re.compile('[A-ZА-Я]{1}[a-zа-я]{1,}\\s{1}[A-ZА-Я]{1}[.]{1}\\s?[A-ZА-Я]{1}[.]{1}|[A-ZА-Я]{1}[.]'), 'End': 1}\n",
    "\n",
    "counter = -1\n",
    "for m in FIO['TextDescriptor'].finditer(text):\n",
    "    counter +=1\n",
    "    ListWithDescriptors.append([])\n",
    "    ListWithDescriptors[counter].extend((m.group(), FIO['DescriptorName'], m.start(), m.end()-m.start(), FIO['End'], ''))\n",
    "    string = \"_\" * (m.end()-m.start())\n",
    "    text = text.replace(m.group(), string)\n",
    "print(ListWithDescriptors)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Поиск аббревиатур и сокращений по словарю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Постсинтаксический анализ решает две следующие задачи. Первая заключается в необходимости уточнить смысл слов, который выражается с помощью различных средств языка: предлогов, префиксов и ____ Проблематика второй задачи заключается в том, что одну и ту же мысль можно выразить разными конструкциями языка. Например, в многоязычно диалоговой системе одна и та же мысль может быть выражена различными синтаксическими конструкциями. В связи с этим появляется необходимость в нормализации дерева, чтобы свести конструкции, которые выражают одно действие различным образом для разных ситуаций, к одному нормализованному дереву. Так же на этапе постсинтаксического анализа может проводиться поиск изменяемых словосочетаний, составные части (слова) которых могут быть разделены другими словами [2].\n",
      "[['т.д.', 'SOKRASCHENIE', 188, 4, 1, 'и так далее']]\n"
     ]
    }
   ],
   "source": [
    "abbreviation=[['МГУ','abr',1,2,'Московский Государственный Университет'],['т.д.','sokr',1,2,'и так далее']]\n",
    "\n",
    "AbbrPatter = {'DescriptorName': 'ABBREVIATION','TextDescriptor': re.compile('[А-Я]{2,}'), 'End': 0}\n",
    "SokrPattern = {'DescriptorName': 'SOKRASCHENIE','TextDescriptor': re.compile('[А-Яа-я]+\\.(?:[А-Яа-я]*\\.|[А-Яа-я]*\\.[А-Яа-я]*\\.)'), 'End': 1}\n",
    "\n",
    "ListWithPatterns=[AbbrPatter, SokrPattern]\n",
    "\n",
    "for pattern in ListWithPatterns:\n",
    "    for m in pattern['TextDescriptor'].finditer(text):\n",
    "        for abbr in abbreviation:\n",
    "            if abbr[0] == m.group():\n",
    "                counter +=1\n",
    "                ListWithDescriptors.append([])\n",
    "                ListWithDescriptors[counter].extend((m.group(), pattern['DescriptorName'], m.start(), m.end()-m.start(), pattern['End'], abbr[4]))\n",
    "                string = \"_\" * (m.end()-m.start())\n",
    "                text = text.replace(m.group(), string)   \n",
    "print(text)\n",
    "print(ListWithDescriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['т.д.', 'SOKRASCHENIE', 188, 4, 1, 'и так далее'], [' ', 'SPACE', 18, 1, 1, ''], [' ', 'SPACE', 25, 1, 1, ''], [' ', 'SPACE', 32, 1, 1, ''], [' ', 'SPACE', 36, 1, 1, ''], [' ', 'SPACE', 46, 1, 1, ''], [' ', 'SPACE', 54, 1, 1, ''], [' ', 'SPACE', 61, 1, 1, ''], [' ', 'SPACE', 73, 1, 1, ''], [' ', 'SPACE', 75, 1, 1, ''], [' ', 'SPACE', 89, 1, 1, ''], [' ', 'SPACE', 98, 1, 1, ''], [' ', 'SPACE', 104, 1, 1, ''], [' ', 'SPACE', 110, 1, 1, ''], [' ', 'SPACE', 118, 1, 1, ''], [' ', 'SPACE', 129, 1, 1, ''], [' ', 'SPACE', 131, 1, 1, ''], [' ', 'SPACE', 139, 1, 1, ''], [' ', 'SPACE', 149, 1, 1, ''], [' ', 'SPACE', 157, 1, 1, ''], [' ', 'SPACE', 164, 1, 1, ''], [' ', 'SPACE', 175, 1, 1, ''], [' ', 'SPACE', 185, 1, 1, ''], [' ', 'SPACE', 187, 1, 1, ''], [' ', 'SPACE', 192, 1, 1, ''], [' ', 'SPACE', 205, 1, 1, ''], [' ', 'SPACE', 212, 1, 1, ''], [' ', 'SPACE', 219, 1, 1, ''], [' ', 'SPACE', 231, 1, 1, ''], [' ', 'SPACE', 233, 1, 1, ''], [' ', 'SPACE', 238, 1, 1, ''], [' ', 'SPACE', 242, 1, 1, ''], [' ', 'SPACE', 247, 1, 1, ''], [' ', 'SPACE', 249, 1, 1, ''], [' ', 'SPACE', 252, 1, 1, ''], [' ', 'SPACE', 255, 1, 1, ''], [' ', 'SPACE', 261, 1, 1, ''], [' ', 'SPACE', 267, 1, 1, ''], [' ', 'SPACE', 276, 1, 1, ''], [' ', 'SPACE', 284, 1, 1, ''], [' ', 'SPACE', 298, 1, 1, ''], [' ', 'SPACE', 305, 1, 1, ''], [' ', 'SPACE', 315, 1, 1, ''], [' ', 'SPACE', 317, 1, 1, ''], [' ', 'SPACE', 329, 1, 1, ''], [' ', 'SPACE', 340, 1, 1, ''], [' ', 'SPACE', 348, 1, 1, ''], [' ', 'SPACE', 353, 1, 1, ''], [' ', 'SPACE', 355, 1, 1, ''], [' ', 'SPACE', 358, 1, 1, ''], [' ', 'SPACE', 361, 1, 1, ''], [' ', 'SPACE', 367, 1, 1, ''], [' ', 'SPACE', 373, 1, 1, ''], [' ', 'SPACE', 378, 1, 1, ''], [' ', 'SPACE', 387, 1, 1, ''], [' ', 'SPACE', 398, 1, 1, ''], [' ', 'SPACE', 414, 1, 1, ''], [' ', 'SPACE', 429, 1, 1, ''], [' ', 'SPACE', 431, 1, 1, ''], [' ', 'SPACE', 437, 1, 1, ''], [' ', 'SPACE', 439, 1, 1, ''], [' ', 'SPACE', 444, 1, 1, ''], [' ', 'SPACE', 455, 1, 1, ''], [' ', 'SPACE', 469, 1, 1, ''], [' ', 'SPACE', 471, 1, 1, ''], [' ', 'SPACE', 484, 1, 1, ''], [' ', 'SPACE', 492, 1, 1, ''], [' ', 'SPACE', 498, 1, 1, ''], [' ', 'SPACE', 505, 1, 1, ''], [' ', 'SPACE', 518, 1, 1, ''], [' ', 'SPACE', 526, 1, 1, ''], [' ', 'SPACE', 535, 1, 1, ''], [' ', 'SPACE', 540, 1, 1, ''], [' ', 'SPACE', 549, 1, 1, ''], [' ', 'SPACE', 559, 1, 1, ''], [' ', 'SPACE', 567, 1, 1, ''], [' ', 'SPACE', 571, 1, 1, ''], [' ', 'SPACE', 578, 1, 1, ''], [' ', 'SPACE', 588, 1, 1, ''], [' ', 'SPACE', 590, 1, 1, ''], [' ', 'SPACE', 597, 1, 1, ''], [' ', 'SPACE', 614, 1, 1, ''], [' ', 'SPACE', 622, 1, 1, ''], [' ', 'SPACE', 626, 1, 1, ''], [' ', 'SPACE', 629, 1, 1, ''], [' ', 'SPACE', 632, 1, 1, ''], [' ', 'SPACE', 638, 1, 1, ''], [' ', 'SPACE', 658, 1, 1, ''], [' ', 'SPACE', 666, 1, 1, ''], [' ', 'SPACE', 672, 1, 1, ''], [' ', 'SPACE', 684, 1, 1, ''], [' ', 'SPACE', 690, 1, 1, ''], [' ', 'SPACE', 701, 1, 1, ''], [' ', 'SPACE', 717, 1, 1, ''], [' ', 'SPACE', 727, 1, 1, ''], [' ', 'SPACE', 733, 1, 1, ''], [' ', 'SPACE', 741, 1, 1, ''], [' ', 'SPACE', 749, 1, 1, ''], [' ', 'SPACE', 755, 1, 1, ''], [' ', 'SPACE', 760, 1, 1, ''], [' ', 'SPACE', 770, 1, 1, ''], [' ', 'SPACE', 778, 1, 1, ''], [' ', 'SPACE', 786, 1, 1, '']]\n",
      "Постсинтаксический анализ решает две следующие задачи. Первая заключается в необходимости уточнить смысл слов, который выражается с помощью различных средств языка: предлогов, префиксов и ____ Проблематика второй задачи заключается в том, что одну и ту же мысль можно выразить разными конструкциями языка. Например, в многоязычно диалоговой системе одна и та же мысль может быть выражена различными синтаксическими конструкциями. В связи с этим появляется необходимость в нормализации дерева, чтобы свести конструкции, которые выражают одно действие различным образом для разных ситуаций, к одному нормализованному дереву. Так же на этапе постсинтаксического анализа может проводиться поиск изменяемых словосочетаний, составные части (слова) которых могут быть разделены другими словами [2].\n"
     ]
    }
   ],
   "source": [
    "SPACE = {'DescriptorName': 'SPACE','TextDescriptor': re.compile(' '), 'End': 0}\n",
    "\n",
    "for m in SPACE['TextDescriptor'].finditer(text):\n",
    "    counter +=1\n",
    "    ListWithDescriptors.append([])\n",
    "    ListWithDescriptors[counter].extend((m.group(), SPACE['DescriptorName'], m.start(), m.end()-m.start(), pattern['End'], ''))\n",
    "    \n",
    "print(ListWithDescriptors)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение остальных шаблонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Постсинтаксический', 'RUSSIAN_LEXEM', 0, 18, 0, ''], [' ', 'SPACE', 18, 1, 1, ''], ['анализ', 'RUSSIAN_LEXEM', 19, 6, 0, ''], [' ', 'SPACE', 25, 1, 1, ''], ['решает', 'RUSSIAN_LEXEM', 26, 6, 0, ''], [' ', 'SPACE', 32, 1, 1, ''], ['две', 'RUSSIAN_LEXEM', 33, 3, 0, ''], [' ', 'SPACE', 36, 1, 1, ''], ['следующие', 'RUSSIAN_LEXEM', 37, 9, 0, ''], [' ', 'SPACE', 46, 1, 1, ''], ['задачи', 'RUSSIAN_LEXEM', 47, 6, 0, ''], ['.', 'END_SYMBOL', 53, 1, 1, ''], [' ', 'SPACE', 54, 1, 1, ''], ['Первая', 'RUSSIAN_LEXEM', 55, 6, 0, ''], [' ', 'SPACE', 61, 1, 1, ''], ['заключается', 'RUSSIAN_LEXEM', 62, 11, 0, ''], [' ', 'SPACE', 73, 1, 1, ''], ['в', 'RUSSIAN_LEXEM', 74, 1, 0, ''], [' ', 'SPACE', 75, 1, 1, ''], ['необходимости', 'RUSSIAN_LEXEM', 76, 13, 0, ''], [' ', 'SPACE', 89, 1, 1, ''], ['уточнить', 'RUSSIAN_LEXEM', 90, 8, 0, ''], [' ', 'SPACE', 98, 1, 1, ''], ['смысл', 'RUSSIAN_LEXEM', 99, 5, 0, ''], [' ', 'SPACE', 104, 1, 1, ''], ['слов', 'RUSSIAN_LEXEM', 105, 4, 0, ''], [',', 'OTHERSYMBOL', 109, 1, 0, ''], [' ', 'SPACE', 110, 1, 1, ''], ['который', 'RUSSIAN_LEXEM', 111, 7, 0, ''], [' ', 'SPACE', 118, 1, 1, ''], ['выражается', 'RUSSIAN_LEXEM', 119, 10, 0, ''], [' ', 'SPACE', 129, 1, 1, ''], ['с', 'RUSSIAN_LEXEM', 130, 1, 0, ''], [' ', 'SPACE', 131, 1, 1, ''], ['помощью', 'RUSSIAN_LEXEM', 132, 7, 0, ''], [' ', 'SPACE', 139, 1, 1, ''], ['различных', 'RUSSIAN_LEXEM', 140, 9, 0, ''], [' ', 'SPACE', 149, 1, 1, ''], ['средств', 'RUSSIAN_LEXEM', 150, 7, 0, ''], [' ', 'SPACE', 157, 1, 1, ''], ['языка', 'RUSSIAN_LEXEM', 158, 5, 0, ''], [':', 'OTHERSYMBOL', 163, 1, 0, ''], [' ', 'SPACE', 164, 1, 1, ''], ['предлогов', 'RUSSIAN_LEXEM', 165, 9, 0, ''], [',', 'OTHERSYMBOL', 174, 1, 0, ''], [' ', 'SPACE', 175, 1, 1, ''], ['префиксов', 'RUSSIAN_LEXEM', 176, 9, 0, ''], [' ', 'SPACE', 185, 1, 1, ''], ['и', 'RUSSIAN_LEXEM', 186, 1, 0, ''], [' ', 'SPACE', 187, 1, 1, ''], ['т.д.', 'SOKRASCHENIE', 188, 4, 1, 'и так далее'], ['____', 'OTHERSYMBOL', 188, 4, 0, ''], [' ', 'SPACE', 192, 1, 1, ''], ['Проблематика', 'RUSSIAN_LEXEM', 193, 12, 0, ''], [' ', 'SPACE', 205, 1, 1, ''], ['второй', 'RUSSIAN_LEXEM', 206, 6, 0, ''], [' ', 'SPACE', 212, 1, 1, ''], ['задачи', 'RUSSIAN_LEXEM', 213, 6, 0, ''], [' ', 'SPACE', 219, 1, 1, ''], ['заключается', 'RUSSIAN_LEXEM', 220, 11, 0, ''], [' ', 'SPACE', 231, 1, 1, ''], ['в', 'RUSSIAN_LEXEM', 232, 1, 0, ''], [' ', 'SPACE', 233, 1, 1, ''], ['том', 'RUSSIAN_LEXEM', 234, 3, 0, ''], [',', 'OTHERSYMBOL', 237, 1, 0, ''], [' ', 'SPACE', 238, 1, 1, ''], ['что', 'RUSSIAN_LEXEM', 239, 3, 0, ''], [' ', 'SPACE', 242, 1, 1, ''], ['одну', 'RUSSIAN_LEXEM', 243, 4, 0, ''], [' ', 'SPACE', 247, 1, 1, ''], ['и', 'RUSSIAN_LEXEM', 248, 1, 0, ''], [' ', 'SPACE', 249, 1, 1, ''], ['ту', 'RUSSIAN_LEXEM', 250, 2, 0, ''], [' ', 'SPACE', 252, 1, 1, ''], ['же', 'RUSSIAN_LEXEM', 253, 2, 0, ''], [' ', 'SPACE', 255, 1, 1, ''], ['мысль', 'RUSSIAN_LEXEM', 256, 5, 0, ''], [' ', 'SPACE', 261, 1, 1, ''], ['можно', 'RUSSIAN_LEXEM', 262, 5, 0, ''], [' ', 'SPACE', 267, 1, 1, ''], ['выразить', 'RUSSIAN_LEXEM', 268, 8, 0, ''], [' ', 'SPACE', 276, 1, 1, ''], ['разными', 'RUSSIAN_LEXEM', 277, 7, 0, ''], [' ', 'SPACE', 284, 1, 1, ''], ['конструкциями', 'RUSSIAN_LEXEM', 285, 13, 0, ''], [' ', 'SPACE', 298, 1, 1, ''], ['языка', 'RUSSIAN_LEXEM', 299, 5, 0, ''], ['.', 'END_SYMBOL', 304, 1, 1, ''], [' ', 'SPACE', 305, 1, 1, ''], ['Например', 'RUSSIAN_LEXEM', 306, 8, 0, ''], [',', 'OTHERSYMBOL', 314, 1, 0, ''], [' ', 'SPACE', 315, 1, 1, ''], ['в', 'RUSSIAN_LEXEM', 316, 1, 0, ''], [' ', 'SPACE', 317, 1, 1, ''], ['многоязычно', 'RUSSIAN_LEXEM', 318, 11, 0, ''], [' ', 'SPACE', 329, 1, 1, ''], ['диалоговой', 'RUSSIAN_LEXEM', 330, 10, 0, ''], [' ', 'SPACE', 340, 1, 1, ''], ['системе', 'RUSSIAN_LEXEM', 341, 7, 0, ''], [' ', 'SPACE', 348, 1, 1, ''], ['одна', 'RUSSIAN_LEXEM', 349, 4, 0, ''], [' ', 'SPACE', 353, 1, 1, ''], ['и', 'RUSSIAN_LEXEM', 354, 1, 0, ''], [' ', 'SPACE', 355, 1, 1, ''], ['та', 'RUSSIAN_LEXEM', 356, 2, 0, ''], [' ', 'SPACE', 358, 1, 1, ''], ['же', 'RUSSIAN_LEXEM', 359, 2, 0, ''], [' ', 'SPACE', 361, 1, 1, ''], ['мысль', 'RUSSIAN_LEXEM', 362, 5, 0, ''], [' ', 'SPACE', 367, 1, 1, ''], ['может', 'RUSSIAN_LEXEM', 368, 5, 0, ''], [' ', 'SPACE', 373, 1, 1, ''], ['быть', 'RUSSIAN_LEXEM', 374, 4, 0, ''], [' ', 'SPACE', 378, 1, 1, ''], ['выражена', 'RUSSIAN_LEXEM', 379, 8, 0, ''], [' ', 'SPACE', 387, 1, 1, ''], ['различными', 'RUSSIAN_LEXEM', 388, 10, 0, ''], [' ', 'SPACE', 398, 1, 1, ''], ['синтаксическими', 'RUSSIAN_LEXEM', 399, 15, 0, ''], [' ', 'SPACE', 414, 1, 1, ''], ['конструкциями', 'RUSSIAN_LEXEM', 415, 13, 0, ''], ['.', 'END_SYMBOL', 428, 1, 1, ''], [' ', 'SPACE', 429, 1, 1, ''], ['В', 'RUSSIAN_LEXEM', 430, 1, 0, ''], [' ', 'SPACE', 431, 1, 1, ''], ['связи', 'RUSSIAN_LEXEM', 432, 5, 0, ''], [' ', 'SPACE', 437, 1, 1, ''], ['с', 'RUSSIAN_LEXEM', 438, 1, 0, ''], [' ', 'SPACE', 439, 1, 1, ''], ['этим', 'RUSSIAN_LEXEM', 440, 4, 0, ''], [' ', 'SPACE', 444, 1, 1, ''], ['появляется', 'RUSSIAN_LEXEM', 445, 10, 0, ''], [' ', 'SPACE', 455, 1, 1, ''], ['необходимость', 'RUSSIAN_LEXEM', 456, 13, 0, ''], [' ', 'SPACE', 469, 1, 1, ''], ['в', 'RUSSIAN_LEXEM', 470, 1, 0, ''], [' ', 'SPACE', 471, 1, 1, ''], ['нормализации', 'RUSSIAN_LEXEM', 472, 12, 0, ''], [' ', 'SPACE', 484, 1, 1, ''], ['дерева', 'RUSSIAN_LEXEM', 485, 6, 0, ''], [',', 'OTHERSYMBOL', 491, 1, 0, ''], [' ', 'SPACE', 492, 1, 1, ''], ['чтобы', 'RUSSIAN_LEXEM', 493, 5, 0, ''], [' ', 'SPACE', 498, 1, 1, ''], ['свести', 'RUSSIAN_LEXEM', 499, 6, 0, ''], [' ', 'SPACE', 505, 1, 1, ''], ['конструкции', 'RUSSIAN_LEXEM', 506, 11, 0, ''], [',', 'OTHERSYMBOL', 517, 1, 0, ''], [' ', 'SPACE', 518, 1, 1, ''], ['которые', 'RUSSIAN_LEXEM', 519, 7, 0, ''], [' ', 'SPACE', 526, 1, 1, ''], ['выражают', 'RUSSIAN_LEXEM', 527, 8, 0, ''], [' ', 'SPACE', 535, 1, 1, ''], ['одно', 'RUSSIAN_LEXEM', 536, 4, 0, ''], [' ', 'SPACE', 540, 1, 1, ''], ['действие', 'RUSSIAN_LEXEM', 541, 8, 0, ''], [' ', 'SPACE', 549, 1, 1, ''], ['различным', 'RUSSIAN_LEXEM', 550, 9, 0, ''], [' ', 'SPACE', 559, 1, 1, ''], ['образом', 'RUSSIAN_LEXEM', 560, 7, 0, ''], [' ', 'SPACE', 567, 1, 1, ''], ['для', 'RUSSIAN_LEXEM', 568, 3, 0, ''], [' ', 'SPACE', 571, 1, 1, ''], ['разных', 'RUSSIAN_LEXEM', 572, 6, 0, ''], [' ', 'SPACE', 578, 1, 1, ''], ['ситуаций', 'RUSSIAN_LEXEM', 579, 8, 0, ''], [',', 'OTHERSYMBOL', 587, 1, 0, ''], [' ', 'SPACE', 588, 1, 1, ''], ['к', 'RUSSIAN_LEXEM', 589, 1, 0, ''], [' ', 'SPACE', 590, 1, 1, ''], ['одному', 'RUSSIAN_LEXEM', 591, 6, 0, ''], [' ', 'SPACE', 597, 1, 1, ''], ['нормализованному', 'RUSSIAN_LEXEM', 598, 16, 0, ''], [' ', 'SPACE', 614, 1, 1, ''], ['дереву', 'RUSSIAN_LEXEM', 615, 6, 0, ''], ['.', 'END_SYMBOL', 621, 1, 1, ''], [' ', 'SPACE', 622, 1, 1, ''], ['Так', 'RUSSIAN_LEXEM', 623, 3, 0, ''], [' ', 'SPACE', 626, 1, 1, ''], ['же', 'RUSSIAN_LEXEM', 627, 2, 0, ''], [' ', 'SPACE', 629, 1, 1, ''], ['на', 'RUSSIAN_LEXEM', 630, 2, 0, ''], [' ', 'SPACE', 632, 1, 1, ''], ['этапе', 'RUSSIAN_LEXEM', 633, 5, 0, ''], [' ', 'SPACE', 638, 1, 1, ''], ['постсинтаксического', 'RUSSIAN_LEXEM', 639, 19, 0, ''], [' ', 'SPACE', 658, 1, 1, ''], ['анализа', 'RUSSIAN_LEXEM', 659, 7, 0, ''], [' ', 'SPACE', 666, 1, 1, ''], ['может', 'RUSSIAN_LEXEM', 667, 5, 0, ''], [' ', 'SPACE', 672, 1, 1, ''], ['проводиться', 'RUSSIAN_LEXEM', 673, 11, 0, ''], [' ', 'SPACE', 684, 1, 1, ''], ['поиск', 'RUSSIAN_LEXEM', 685, 5, 0, ''], [' ', 'SPACE', 690, 1, 1, ''], ['изменяемых', 'RUSSIAN_LEXEM', 691, 10, 0, ''], [' ', 'SPACE', 701, 1, 1, ''], ['словосочетаний', 'RUSSIAN_LEXEM', 702, 14, 0, ''], [',', 'OTHERSYMBOL', 716, 1, 0, ''], [' ', 'SPACE', 717, 1, 1, ''], ['составные', 'RUSSIAN_LEXEM', 718, 9, 0, ''], [' ', 'SPACE', 727, 1, 1, ''], ['части', 'RUSSIAN_LEXEM', 728, 5, 0, ''], [' ', 'SPACE', 733, 1, 1, ''], ['(', 'OPEN_BRACKET', 734, 1, 0, ''], ['слова', 'RUSSIAN_LEXEM', 735, 5, 0, ''], [')', 'CLOSE_BRACKET', 740, 1, 0, ''], [' ', 'SPACE', 741, 1, 1, ''], ['которых', 'RUSSIAN_LEXEM', 742, 7, 0, ''], [' ', 'SPACE', 749, 1, 1, ''], ['могут', 'RUSSIAN_LEXEM', 750, 5, 0, ''], [' ', 'SPACE', 755, 1, 1, ''], ['быть', 'RUSSIAN_LEXEM', 756, 4, 0, ''], [' ', 'SPACE', 760, 1, 1, ''], ['разделены', 'RUSSIAN_LEXEM', 761, 9, 0, ''], [' ', 'SPACE', 770, 1, 1, ''], ['другими', 'RUSSIAN_LEXEM', 771, 7, 0, ''], [' ', 'SPACE', 778, 1, 1, ''], ['словами', 'RUSSIAN_LEXEM', 779, 7, 0, ''], [' ', 'SPACE', 786, 1, 1, ''], ['[', 'OPEN_BRACKET', 787, 1, 0, ''], ['2', 'NUMBER', 788, 1, 0, ''], [']', 'CLOSE_BRACKET', 789, 1, 0, ''], ['.', 'END_SYMBOL', 790, 1, 1, '']]\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "MAIL = {'DescriptorName': 'MAIL','TextDescriptor': re.compile('\\\\b[^\\s]+@(?:[a-z]{1,4}\\.[a-z]{1,4}\\.[a-z]{1,4}|[a-z]{1,4}\\.[a-z]{1,4})\\\\b'), 'End': 0}\n",
    "URL = {'DescriptorName': 'URL','TextDescriptor': re.compile('(?:(?:https://|http://)|www.){1}[^\\s]+[^\\s?!.]'), 'End': 0}\n",
    "DATE = {'DescriptorName': 'DATE','TextDescriptor': re.compile('[0-9]{1,2}[/:.][0-9]{1,2}[/:.][1-9]{1}(?:[0-9]{3}|[0-9]{1})'), 'End': 1}\n",
    "PHONE = {'DescriptorName': 'PHONE','TextDescriptor': re.compile('\\+?[0-9]{0,2}-?\\(?[0-9]{3,5}\\)?-?[0-9]{2,3}-?[0-9]{2}-?[0-9]{2}'), 'End': 0}\n",
    "ListWithSpecialPatterns = [MAIL, URL, DATE, PHONE]\n",
    "\n",
    "CLOSE_QUOTE = {'DescriptorName': 'CLOSE_QUOTE','TextDescriptor': re.compile('\\\\b[^ $]+([\\'\\\"\\>\\»])(?:$|[^\\w\\d\\-\\(\\[\\{]{1,3})'), 'End': 0}\n",
    "CLOSE_BRACKET = {'DescriptorName': 'CLOSE_BRACKET','TextDescriptor': re.compile('\\\\b[^ $]+([\\)\\]\\}])(?:$|[^\\w\\d\\-\\(\\[\\{]{1,3})'), 'End': 0}\n",
    "END_SYMBOL = {'DescriptorName': 'END_SYMBOL','TextDescriptor': re.compile('((?:\\.\\.\\.|[!?\\.]))(?:$| )'), 'End': 1}\n",
    "DELIMETER = {'DescriptorName': 'DELIMETER','TextDescriptor': re.compile('\\\\b[^ \\:\\;\\,\\.\\,\\-\\!\\?\\(\\[\\{]([\\:\\;\\,]) '), 'End': 0}\n",
    "OPEN_QUOTE = {'DescriptorName': 'OPEN_QUOTE','TextDescriptor': re.compile('[ \\(\\[\\{]([\\'\\\"\\<\\«])[ ёА-Яа-я0-9+\\(\\[\\{]'), 'End': 0}\n",
    "OPEN_BRACKET = {'DescriptorName': 'OPEN_BRACKET','TextDescriptor': re.compile('(?:^|[ \\(\\[\\{\\'\\\"\\<])([\\{\\[\\(])[ ёА-Яа-я0-9+\\(\\[\\{]'), 'End': 0}\n",
    "ListWithNonSpaceSymbolPatterns = [CLOSE_QUOTE, CLOSE_BRACKET, END_SYMBOL, DELIMETER, OPEN_QUOTE, OPEN_BRACKET]\n",
    "\n",
    "RUSSIAN_LEXEM = {'DescriptorName': 'RUSSIAN_LEXEM','TextDescriptor': re.compile('\\\\b([ёа-яА-Я]+)\\\\b'), 'End': 0}\n",
    "NUMBER = {'DescriptorName': 'NUMBER','TextDescriptor': re.compile('\\\\b[0-9]+\\\\b'), 'End': 0}\n",
    "OTHERSYMBOL = {'DescriptorName': 'OTHERSYMBOL', 'TextDescriptor': re.compile('[^ ]+'), 'End': 0}\n",
    "ListWithOtherPatterns = [RUSSIAN_LEXEM, NUMBER, OTHERSYMBOL]\n",
    "\n",
    "for pattern in ListWithSpecialPatterns:\n",
    "    for m in pattern['TextDescriptor'].finditer(text):\n",
    "        counter += 1\n",
    "        ListWithDescriptors.append([])\n",
    "        ListWithDescriptors[counter].extend((m.group(), pattern['DescriptorName'], m.start(), 1, pattern['End'], ''))\n",
    "        string = \" \" * (m.end()-m.start())\n",
    "        text=text.replace(m.group(), string, 1)\n",
    "\n",
    "for pattern in ListWithNonSpaceSymbolPatterns:\n",
    "    for m in pattern['TextDescriptor'].finditer(text):\n",
    "        counter += 1\n",
    "        ListWithDescriptors.append([])\n",
    "        ListWithDescriptors[counter].extend((m.group(1), pattern['DescriptorName'], m.start(1), 1, pattern['End'], ''))\n",
    "        text=text.replace(m.group(1), ' ', 1)\n",
    "\n",
    "for pattern in ListWithOtherPatterns:\n",
    "    for m in pattern['TextDescriptor'].finditer(text):\n",
    "        counter += 1\n",
    "        ListWithDescriptors.append([])\n",
    "        ListWithDescriptors[counter].extend((m.group(), pattern['DescriptorName'], m.start(), m.end()-m.start(), pattern['End'], ''))\n",
    "        string = \" \" * (m.end()-m.start())\n",
    "        text=text.replace(m.group(),string,1)\n",
    "\n",
    "print(sorted(ListWithDescriptors, key=lambda x: x[2]))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение предложений в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step1(text):\n",
    "    ListWithSentenceCharacteristic = []\n",
    "    counter = -1\n",
    "    text_original = text\n",
    "    SentenceStartPos = re.search('\\S', text).start()\n",
    "    step2(SentenceStartPos, text, ListWithSentenceCharacteristic, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Шаг 2 Находим в тексте первую последовательность завершающих симвлов (SentenceEndSeq), следующую за SentenceStartPos \n",
    "def step2(SentenceStartPos, text, ListWithSentenceCharacteristic, counter): \n",
    "    SentenceEndSeq='\\?{1,}|\\!{1,}|\\.{1,}'\n",
    "    # Если такая последовательность найдена, запоминаем её позицию и переходим к следующему шагу\n",
    "    if re.search(SentenceEndSeq, text):\n",
    "        SentenceEndSeqStartPos = re.search(SentenceEndSeq, text).start()\n",
    "        SentenceEndSeqEndPos = re.search(SentenceEndSeq, text).end()\n",
    "        string = \" \" * SentenceEndSeqEndPos\n",
    "        text=text.replace(text[:SentenceEndSeqEndPos],string,1)\n",
    "        step3(SentenceStartPos, SentenceEndSeqEndPos, text, ListWithSentenceCharacteristic, counter)   \n",
    "    else:\n",
    "        # Если такая последовательность не найдена, и в процессе поиска достигнут конец файла, \n",
    "        # то присваиваем метку конца предложения последнему символу в тексте и переходим к шагу 5\n",
    "        counter += 1\n",
    "        ListWithSentenceCharacteristic.append([])\n",
    "        print(text_original[SentenceStartPos:(len(text)-SentenceStartPos)])\n",
    "        ListWithSentenceCharacteristic[counter].extend((SentenceStartPos, len(text)-SentenceStartPos))\n",
    "        step4(ListWithSentenceCharacteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step3(SentenceStartPos, SentenceEndSeqEndPos, text, ListWithSentenceCharacteristic, counter):\n",
    "    # После нахождения последовательности SentenceEndSeq ищем первый не пробельный символ, следующий за SentenceEndSeq\n",
    "    if re.search('\\S', text):\n",
    "        # Если следующий за SentenceEndSeq не пробельный символ является заглавной буквой, то найден предположительный конец \n",
    "        # предложения, снова переходим к шагу 2\n",
    "        if re.search('[А-Я]', re.search('\\S', text).group()):\n",
    "            counter += 1\n",
    "            ListWithSentenceCharacteristic.append([])\n",
    "            ListWithSentenceCharacteristic[counter].extend((SentenceStartPos, SentenceEndSeqEndPos-SentenceStartPos))\n",
    "            step2(SentenceEndSeqEndPos, text, ListWithSentenceCharacteristic, counter)\n",
    "        else:\n",
    "        # иначе переходим к шагу 2 и начинаем искать следующую последовательность SentenceEndSeq\n",
    "            step2(SentenceStartPos, text, ListWithSentenceCharacteristic, counter)\n",
    "            \n",
    "    # Если в процессе поиска достигнут конец файла, то присваиваем метку SentenceEndPos последнему символу из SentenceEndSeq \n",
    "    # и переходим к шагу 4\n",
    "    else:\n",
    "        counter += 1\n",
    "        ListWithSentenceCharacteristic.append([])\n",
    "        ListWithSentenceCharacteristic[counter].extend((SentenceStartPos, len(text)-SentenceStartPos))\n",
    "        step4(ListWithSentenceCharacteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step4(ListWithSentenceCharacteristic):\n",
    "    print(ListWithSentenceCharacteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = 'Постсинтаксический анализ решает две следующие задачи. Первая заключается в необходимости уточнить смысл слов, который выражается с помощью различных средств языка: предлогов, префиксов и т.д. Проблематика второй задачи заключается в том, что одну и ту же мысль можно выразить разными конструкциями языка. Например, в многоязычно диалоговой системе одна и та же мысль может быть выражена различными синтаксическими конструкциями. В связи с этим появляется необходимость в нормализации дерева, чтобы свести конструкции, которые выражают одно действие различным образом для разных ситуаций, к одному нормализованному дереву. Так же на этапе постсинтаксического анализа может проводиться поиск изменяемых словосочетаний, составные части (слова) которых могут быть разделены другими словами [2].'\n",
    "step1(text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
